{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bnns on tensorflow- mnist",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tucanae47/bnns/blob/master/Bnns_on_tensorflow_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BlmQIFSLZDdc"
      },
      "cell_type": "markdown",
      "source": [
        "# From [uranus bnn on tf](https://github.com/uranusx86/BinaryNet-on-tensorflow)\n",
        "\n",
        "\n",
        "binary weight neural network implementation on tensorflow\n",
        "\n",
        "This is an implementation code for reproducing BNN\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3IEVK-KFxi5Z",
        "outputId": "9cafca0b-ffe7-406f-a54b-2aa9ec3ef90c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TQWLP2zMTmg8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import binary_layer as binary\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "from mnist import download_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZUPrNeEpWIp_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Main functions"
      ]
    },
    {
      "metadata": {
        "id": "cORfezIAWLq-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fully_connect_bn(pre_layer, output_dim, act, use_bias, training):\n",
        "    pre_act = binary.dense_binary(pre_layer, output_dim,\n",
        "                                    use_bias = use_bias,\n",
        "                                    activation = None,\n",
        "                                    kernel_constraint = lambda w: tf.clip_by_value(w, -1.0, 1.0))\n",
        "    bn = binary.batch_normalization(pre_act, momentum=0.9, epsilon=1e-4, training=training)\n",
        "    if act == None:\n",
        "        output = bn\n",
        "    else:\n",
        "        output = act(bn)\n",
        "    return output\n",
        "\n",
        "def no_scale_dropout(pre_layer, drop_rate, training):\n",
        "    drop_layer = tf.layers.dropout(pre_layer, rate=drop_rate, training=training)\n",
        "    #return tf.cond(training, lambda: drop_layer*(1-drop_rate), lambda: drop_layer)\n",
        "    return drop_layer\n",
        "\n",
        "# A function which shuffles a dataset\n",
        "def shuffle(X,y):\n",
        "    print(len(X))\n",
        "    shuffle_parts = 1\n",
        "    chunk_size = int(len(X)/shuffle_parts)\n",
        "    shuffled_range = np.arange(chunk_size)\n",
        "\n",
        "    X_buffer = np.copy(X[0:chunk_size])\n",
        "    y_buffer = np.copy(y[0:chunk_size])\n",
        "\n",
        "    for k in range(shuffle_parts):\n",
        "\n",
        "        np.random.shuffle(shuffled_range)\n",
        "\n",
        "        for i in range(chunk_size):\n",
        "\n",
        "            X_buffer[i] = X[k*chunk_size+shuffled_range[i]]\n",
        "            y_buffer[i] = y[k*chunk_size+shuffled_range[i]]\n",
        "\n",
        "        X[k*chunk_size:(k+1)*chunk_size] = X_buffer\n",
        "        y[k*chunk_size:(k+1)*chunk_size] = y_buffer\n",
        "\n",
        "    return X,y\n",
        "\n",
        "# This function trains the model a full epoch (on the whole dataset)\n",
        "def train_epoch(X, y, sess, batch_size=100):\n",
        "    batches = int(len(X)/batch_size)\n",
        "    for i in range(batches):\n",
        "        sess.run([train_kernel_op, train_other_op],\n",
        "            feed_dict={ x: X[i*batch_size:(i+1)*batch_size],\n",
        "                        target: y[i*batch_size:(i+1)*batch_size],\n",
        "                        training: True})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D6fvsGBMWYw2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "data sets download "
      ]
    },
    {
      "metadata": {
        "id": "HPsZdYZdWVEr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "169809f1-715b-47ce-908b-56d1502a8a7a"
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "target = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "training = tf.placeholder(tf.bool)\n",
        "\n",
        "download_mnist.maybe_download('./mnist/MNIST_data/')\n",
        "mnist = input_data.read_data_sets('./mnist/MNIST_data/', one_hot=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading MNIST data...\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz... Done\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz... Done\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz... Done\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz... Done\n",
            "Done downloading MNIST\n",
            "WARNING:tensorflow:From <ipython-input-4-2914c73ab2c1>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./mnist/MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./mnist/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting ./mnist/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AIPei_8sW-hA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "class vectors to binary"
      ]
    },
    {
      "metadata": {
        "id": "_8Ka5DsIW_KX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "babe4e57-afef-4f44-ecad-641c14f748ad"
      },
      "cell_type": "code",
      "source": [
        "# convert class vectors to binary class vectors\n",
        "for i in range(mnist.train.images.shape[0]):\n",
        "    mnist.train.images[i] = mnist.train.images[i] * 2 - 1\n",
        "for i in range(mnist.test.images.shape[0]):\n",
        "    mnist.test.images[i] = mnist.test.images[i] * 2 - 1\n",
        "for i in range(mnist.train.labels.shape[0]):\n",
        "    mnist.train.labels[i] = mnist.train.labels[i] * 2 - 1 # -1 or 1 for hinge loss\n",
        "for i in range(mnist.test.labels.shape[0]):\n",
        "    mnist.test.labels[i] = mnist.test.labels[i] * 2 - 1\n",
        "print(mnist.test.labels.shape)\n",
        "print(mnist.test.images.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 10)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m-qEpm-gXr4o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Layers"
      ]
    },
    {
      "metadata": {
        "id": "yWhflUW_Xtag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "615a62be-0b0f-4739-bd04-65b0665027dd"
      },
      "cell_type": "code",
      "source": [
        "layer0 = no_scale_dropout(x, drop_rate=0.2, training=training)\n",
        "\n",
        "layer1 = fully_connect_bn(layer0, 4096, act=binary.binary_tanh_unit, use_bias=True, training=training)\n",
        "layer1_dp = no_scale_dropout(layer1, drop_rate=0.5, training=training)\n",
        "\n",
        "layer2 = fully_connect_bn(layer1_dp, 4096, act=binary.binary_tanh_unit, use_bias=True, training=training)\n",
        "layer2_dp = no_scale_dropout(layer2, drop_rate=0.5, training=training)\n",
        "\n",
        "layer3 = fully_connect_bn(layer2_dp, 4096, act=binary.binary_tanh_unit, use_bias=True, training=training)\n",
        "layer3_dp = no_scale_dropout(layer3, drop_rate=0.5, training=training)\n",
        "\n",
        "layer4 = fully_connect_bn(layer3_dp, 10, act=None, use_bias=True, training=training)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4096\n",
            "('H = ', 1.0)\n",
            "('LR scale = ', 57.038)\n",
            "('shape: ', 2)\n",
            "4096\n",
            "('H = ', 1.0)\n",
            "('LR scale = ', 73.90083)\n",
            "('shape: ', 2)\n",
            "4096\n",
            "('H = ', 1.0)\n",
            "('LR scale = ', 73.90083)\n",
            "('shape: ', 2)\n",
            "10\n",
            "('H = ', 1.0)\n",
            "('LR scale = ', 52.31953)\n",
            "('shape: ', 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yE9M1qjhZFlh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "train"
      ]
    },
    {
      "metadata": {
        "id": "7ZDEkB8lZGby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "65bd30f8-84f8-40ab-9159-45159954131a"
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(tf.square(tf.maximum(0.,1.-target*layer4)))\n",
        "\n",
        "epochs = 1000\n",
        "lr_start = 0.003\n",
        "lr_end = 0.0000003\n",
        "lr_decay = (lr_end / lr_start)**(1. / epochs)\n",
        "global_step1 = tf.Variable(0, trainable=False)\n",
        "global_step2 = tf.Variable(0, trainable=False)\n",
        "lr1 = tf.train.exponential_decay(lr_start, global_step=global_step1, decay_steps=int(mnist.train.images.shape[0]/100), decay_rate=lr_decay)\n",
        "lr2 = tf.train.exponential_decay(lr_start, global_step=global_step2, decay_steps=int(mnist.train.images.shape[0]/100), decay_rate=lr_decay)\n",
        "\n",
        "sess = tf.Session()\n",
        "saver = tf.train.Saver()\n",
        "#saver.restore(sess, \"model/model.ckpt\")\n",
        "\n",
        "other_var = [var for var in tf.trainable_variables() if not var.name.endswith('kernel:0')]\n",
        "opt = binary.AdamOptimizer(binary.get_all_LR_scale(), lr1)\n",
        "opt2 = tf.train.AdamOptimizer(lr2)\n",
        "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "with tf.control_dependencies(update_ops):   # when training, the moving_mean and moving_variance in the BN need to be updated.\n",
        "    train_kernel_op = opt.apply_gradients(binary.compute_grads(loss, opt),  global_step=global_step1)\n",
        "    train_other_op  = opt2.minimize(loss, var_list=other_var,  global_step=global_step2)\n",
        "\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(layer4, 1), tf.argmax(target, 1)), tf.float32))\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "old_acc = 0.0\n",
        "X_train, y_train = shuffle(mnist.train.images, mnist.train.labels)\n",
        "for i in range(epochs):\n",
        "    train_epoch(X_train, y_train, sess)\n",
        "    X_train, y_train = shuffle(mnist.train.images, mnist.train.labels)\n",
        "\n",
        "    hist = sess.run([accuracy, opt._lr],\n",
        "                    feed_dict={\n",
        "                        x: mnist.test.images,\n",
        "                        target: mnist.test.labels,\n",
        "                        training: False\n",
        "                    })\n",
        "    print(hist)\n",
        "\n",
        "    if hist[0] > old_acc:\n",
        "        old_acc = hist[0]\n",
        "        save_path = saver.save(sess, \"./mnist/model/model.ckpt\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('grad: ', [(<tf.Tensor 'gradients/dense__binary_layer/div_grad/tuple/control_dependency:0' shape=(784, 4096) dtype=float32>, <tf.Variable 'dense__binary_layer/kernel:0' shape=(784, 4096) dtype=float32_ref>)])\n",
            "('grad: ', [(<tf.Tensor 'gradients_1/dense__binary_layer_1/div_grad/tuple/control_dependency:0' shape=(4096, 4096) dtype=float32>, <tf.Variable 'dense__binary_layer_1/kernel:0' shape=(4096, 4096) dtype=float32_ref>)])\n",
            "('grad: ', [(<tf.Tensor 'gradients_2/dense__binary_layer_2/div_grad/tuple/control_dependency:0' shape=(4096, 4096) dtype=float32>, <tf.Variable 'dense__binary_layer_2/kernel:0' shape=(4096, 4096) dtype=float32_ref>)])\n",
            "('grad: ', [(<tf.Tensor 'gradients_3/dense__binary_layer_3/div_grad/tuple/control_dependency:0' shape=(4096, 10) dtype=float32>, <tf.Variable 'dense__binary_layer_3/kernel:0' shape=(4096, 10) dtype=float32_ref>)])\n",
            "[<tf.Tensor 'gradients/dense__binary_layer/div_grad/tuple/control_dependency:0' shape=(784, 4096) dtype=float32>, <tf.Tensor 'gradients_1/dense__binary_layer_1/div_grad/tuple/control_dependency:0' shape=(4096, 4096) dtype=float32>, <tf.Tensor 'gradients_2/dense__binary_layer_2/div_grad/tuple/control_dependency:0' shape=(4096, 4096) dtype=float32>, <tf.Tensor 'gradients_3/dense__binary_layer_3/div_grad/tuple/control_dependency:0' shape=(4096, 10) dtype=float32>]\n",
            "[<tf.Variable 'dense__binary_layer/kernel:0' shape=(784, 4096) dtype=float32_ref>, <tf.Variable 'dense__binary_layer_1/kernel:0' shape=(4096, 4096) dtype=float32_ref>, <tf.Variable 'dense__binary_layer_2/kernel:0' shape=(4096, 4096) dtype=float32_ref>, <tf.Variable 'dense__binary_layer_3/kernel:0' shape=(4096, 10) dtype=float32_ref>]\n",
            "55000\n",
            "55000\n",
            "[0.8786, 0.0029724957]\n",
            "55000\n",
            "[0.8649, 0.0029452438]\n",
            "55000\n",
            "[0.8518, 0.0029182418]\n",
            "55000\n",
            "[0.8573, 0.0028914872]\n",
            "55000\n",
            "[0.8713, 0.002864978]\n",
            "55000\n",
            "[0.8736, 0.0028387117]\n",
            "55000\n",
            "[0.8843, 0.0028126866]\n",
            "55000\n",
            "[0.8838, 0.0027868997]\n",
            "55000\n",
            "[0.896, 0.0027613493]\n",
            "55000\n",
            "[0.9004, 0.0027360332]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}